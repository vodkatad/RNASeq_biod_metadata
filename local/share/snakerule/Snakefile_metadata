# Use snakemake metadata_forall  to check if you have basic metadata available for all the samples in the batches
# that you want to analyze.
include: "./conf.sk"

rule wide_to_long_metadata:
    input: OVERVIEW
    output: "metadata"
    params: tool=BIN_DIR+"/overview2long"
    shell:
        """
            {params.tool} {input} {output}
        """


rule has_metadata:
    input: "metadata"
    output: "has_metadata"
    shell:
        """
            cut -f 2 {input} | sort | uniq > {output}
        """

rule batches:
    input: BATCHES_FASTQS
    output: "fastq_batches"
    run:
        batch = 1
        with open(output[0], 'w') as out:
            for f in input:
                with open(f,'r') as inf:
                    for l in inf.readlines():
                        l = l.rstrip("\n")
                        l = l.replace('_R1.fastq.gz','')
                        out.write(l + "\t" + str(batch) + "\n")
                    batch += 1            

rule info_and_fastq:
    input: meta="metadata", fastq="fastq_batches"
    output: "complete_samples_batch"
    shell:
        """
            cat {input.fastq} | filter_1col 1 <(cut -f 2 {input.meta}) > {output}
        """

rule metadata_forall:
    input: "complete_samples_batch", "fastq_batches"
    shell:
        """
            n1=$(cat {input[0]} | wc -l);
            n2=$(cat {input[1]} | wc -l);
            if [ "$n1" -eq "$n2" ]; then
                echo "Good to go!"
            else
                echo "Go back to square one!"
            fi;
        """

# Note: for batch correction we want the smaller batch a genealogy appears (probably...for library prep batch effect honestly we will never be sure), 
# but for alignment we want the higher one
## Progressive enrichment of metadata
rule metadata_batch:
    input: batch="complete_samples_batch", metadata="metadata"
    output: "metadata_batch"
    shell:
        """
            translate -a -n {input.metadata} 1 < {input.batch} > {output}
        """

# TODO rule add depth? HOW? Ask Franceschino or Claudio for the list.

rule add_cetuxi:
    input: start="metadata_batch", add=DATA_DIR+"/cetuxi_w3.txt"
    output: "metadata_batch_w3vivocetuxi"
    shell:
        """
            bawk '{{print $0,substr($1,0,7)}}' {input.start} | translate -v -e NA {input.add} 4 > {output}
        """

rule add_irino:
    input: start="metadata_batch_w3vivocetuxi", add=DATA_DIR+"/irinotecan_w3.txt"
    output: "metadata_batch_w3vivocetuxi_w3vivoirino"
    shell:
        """
            bawk '{{print $0,substr($1,0,7)}}' {input.start} | translate -v -e NA <(sed 1d {input.add} | bawk '{{print $1,$2/100}}') 5 > {output}
        """

# Will then select all PDO, all PDX and all human samples from the whole list to align all together? Downloading from different places will
# be complicated tough...
rule all_n_samples_comparisons:
    input: expand("batch_{b}.txt", b=list(BATCHES_COUNTS.keys()))

#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ cat batch_?.txt
# TODO manual check on results
rule n_samples_comparisons:
    input: meta="metadata_batch", counts=lambda wildcards: BASE_DIR+"/"+BATCHES_COUNTS[wildcards.b]
    output: "batch_{b}.txt"
    shell:
        """
            bawk '$3=={wildcards.b}' {input.meta} | wc -l > {output}
            zcat {input.counts} | bawk 'NR==1' | cut -f 7-  | tr "\\t" "\\n" | wc -l >> {output}
        """

rule choose_batch: 
    input: meta="metadata_batch"
    output: chosen_batch="merged_batch.tsv"
    run:
        import pandas as pd 
        batch = pd.read_csv(input.meta, sep="\t", header=None) # ERROR ADD HEADER_NONE @!@!!!! TODO
        batch.columns = ['id','class','batch']
        chosen = batch.groupby(['id'])['batch'].max()
        chosen.to_csv(output.chosen_batch, sep="\t")


rule matrix_hs_mm: 
    input: "merged_batch.tsv", [BASE_DIR+"/"+BATCHES_COUNTS[b] for b in list(BATCHES_COUNTS.keys())]
    output: matrix="merged_hs_mm.tsv.gz", info="merged_info.tsv"
    run:
        import pandas as pd 
        batch = pd.read_csv(input[0], sep="\t", header=None)
        batch.columns = ['id','batch']
        res = None
        b = 1
        for count in input[1:]:
            print(count)
            count = pd.read_csv(count, sep="\t", index_col=0)
            wanted = batch[batch['batch'] == b].id
            if b == 1:
                info = count[['Chr','Start','End','Strand','Length']]
            wantedcolumns = count.columns.intersection(wanted)
            #wantedcolumns.to_frame().to_csv(str(b)+"meh.tsv")
            #count.columns.to_frame().to_csv(str(b)+"meh1.tsv")
            #wanted.to_frame().to_csv(str(b)+"meh2.tsv")
            if b != 1:
                 # suffixes=(False, False): raise an exception on overlapping columns
                res = count[wantedcolumns].merge(res, suffixes=(False, False), left_index=True, right_index=True)
            else:
                res = count[wantedcolumns]
            b += 1
        res.to_csv(output.matrix, sep="\t", compression='gzip', header=True)
        info.to_csv(output.info, sep="\t")


rule merged_human:
    input: "merged_hs_mm.tsv.gz"
    output: "merged_hs.tsv.gz"
    shell:
        """
            zcat {input} | bawk 'NR==1' > {output}.tmp
            zcat {input} | grep "^H_" >> {output}.tmp
            gzip -c {output}.tmp > {output}
            rm {output}.tmp
        """

#rule check: done!
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ zcat merged_hs_mm.tsv.gz  | wc -l
#109888
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ zcat merged_hs_mm.tsv.gz  | bawk '{print NF}' |head
#937
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ cut -f 1 metadata_batch_w3vivocetuxi_w3vivoirino  |sort| uniq | wc -l
#938
# FIXED (idiota):
#grassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ zcat merged_hs_mm.tsv.gz  | bawk '{print NF}' |head
#939

rule selected_metadata:
    input: meta="metadata_batch_w3vivocetuxi_w3vivoirino", sel="merged_batch.tsv"
    output: "selected_metadata"
    shell:
        """
            bawk '{{print $1,$3,$2,$4,$5}}' {input.meta} | filter_2col --both-orders 1 2 {input.sel} > {output}
        """

rule gene_len:
    input: "merged_info.tsv"
    output: "gene_len"
    shell:
        """
            cut -f1,6 {input} | sed 's/Length/length/1' > {output}
        """

### Json for Las repoindex ###
#biod id	ID Vale	id	LAS_Validation	clone_N	cloning_date	species
rule json_meta:
    input: metadata="metadata_batch"
    output: "{b}_meta_for_json.tsv"
    shell:
        """
            echo -e "ID\\tLAS_Validation\\tlabel" > {output}
            grep -v '\-' {input.metadata} | bawk '$3=="{wildcards.b}" {{print $1,"TRUE",$2}}' >> {output}
            grep '\-' {input.metadata} | bawk '$3=="{wildcards.b}" {{print $1,"FALSE",$2}}' >> {output}
        """

rule json:
    input: tsv="{b}_meta_for_json.tsv", head=JSON_HEAD+"_{b}.json"
    output: "batch{b}_metadata.json"
    params: tool=BIN_DIR+"/tsv_to_json_metadata"
    shell:
        """
           cp {input.head} {output}
           {params.tool} -i {input.tsv} -j {output}
        """

rule all_json:
    input: expand("batch{b}_metadata.json", b=range(1, len(BATCHES_FASTQS)+1))


    
### lymphomas murine (based on PC) or human (based on scores)
# sort uniq on leuco is needed cause:
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/july2020$ cut -f 1 ../../local/share/data/stromal_contamination.txt | sort | uniq -d
#... there are duplicates with different scores, but not different classifications:
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/july2020$ sed 1d /mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/local/share/data/stromal_contamination.txt | bawk '$3 > 10{print $1,"RNA_marker"} $3 <= 10 {print $1,'pass'}' | sort | uniq | cut -f 1 | sort | uniq -d | wc -l
#0


rule annotation_pc_lympho:
    input: meta='selected_metadata', pcs=PCS, leuco=SCORES
    output: "selected_metadata_values"
    shell:
        """
            translate -a <(sed 1d {input.pcs} | cut -f1,2,3) 1 < {input.meta} > {output}.tmp
            translate -a <(zcat {input.leuco} | sed 1d | cut -f1,4) 1 < {output}.tmp > {output}
        """

## here i plot per la visualizzazione migliore dei thr?
rule pcplot:
    input: "selected_metadata_values"
    params: tool=BIN_DIR+"/Plot_PC"
    output: "PC1_{y}_{color}_{shape}.png"
    shell:
        """
            {params.tool} -i {input} -y {wildcards.y} -c {wildcards.color} -s {wildcards.shape} -o {output}
        """

rule filter_pc_linfo:
    input: "selected_metadata_values"
    output: "selected_metadata_annot"
    params: pc=PCS_2_THR, leuco=LEUCO_THR
    shell:
        """
            cat {input} | cut -f1,2,4,5,6,7,8 | bawk '$3 >= {params.pc} {{print $0,"RNA_PC"}} $3 < {params.pc} {{print $0,"pass"}}' > {output}.tmp
            cat {output}.tmp | bawk '$2 >= {params.leuco} {{print $0,"RNA_marker"}} $2 < {params.leuco} {{print $0,"pass"}}' | bawk '{{print $1,$9,$8,$4,$5,$6,$7}}' | sort | uniq > {output}
        """


### TODO list of lymphomas for Francescone and methy
#Ecco un excel con il matricione nei primo foglio e le annotazioni nel secondo, con una colonna con i linfomi:
#- identificati tramite analisi di PC: RNA_PC
#- identificati con marker analysis da Claudio: RNA_marker
#- secondo metilazione: METHY_L --> registered the new lympho annotation from pdx_methylation PC2>500
#- secondo Fra: FRA_L --> registered the one from the new check of Fra
##- DONE :D
rule add_linfo:
    input: annot="selected_metadata_annot", fra=FRA, marco=MARCO
    output: "selected_metadata_annot_final"
    script: SRC_DIR+"/add_linfo_info.R"


# mark as STRANGE 
#egrassi@godot:/mnt/trcanmed/snaketree/prj/DE_RNASeq/dataset$ grep CRC0164LMX0B /mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/july2020/selected_metadata_annot
#CRC0164LMX0B02202TUMR02000      NA      pass    1       LMX_BASALE      NA      NA
# CRC0385 is not there
# We filter using only RNAseq internal criteria and use FRA and methyl info to evaluate the filters not for actual filtering (if something is bad here and good here we claim heterogeneity)
rule filtered_metadata:
    input: metadata="selected_metadata_annot_final", filter_seqqc='removed_samples_readsQC'
    output: "selected_metadata_annot_final_nolinfo_nooutlier"
    params: outliers="CRC0164LMX0B02202TUMR02000" # this is already removed by the QC but since it's a known rogue...
    shell:
        """
            head -n 1 {input.metadata} > {output}
            sed 1d {input.metadata} | bawk -v strange={params.outliers} '$1 != strange' | bawk '($2 == "pass") && ($3 =="pass")' | filter_1col -v 1 {input.filter_seqqc} >> {output}
        """

####################### roar misplaced ########

#rule mM:
#    input:


#### n. of reads, % assigned on tot
# (bit_rnaseq_2.8) egrassi@godot:/mnt/trcanmed/bioinfotree/prj/RNAseq_biodiversa/dataset/v1$ zless fastq.featurecounts.ribo.ex.count.gz | cut -f 7 | bawk 'BEGIN{n=0}{n=n+$1}END{print n}'
# 8841218
# (bit_rnaseq_2.8) egrassi@godot:/mnt/trcanmed/bioinfotree/prj/RNAseq_biodiversa/dataset/v1$ sed 1d usable_reads  | bawk '{print $1,$7,$7/$2}'  |head -n 1
# CRC0017LMX0A02204TUMR02000      8841218 0.78570110786367797
# (bit_rnaseq_2.8) egrassi@godot:/mnt/trcanmed/bioinfotree/prj/RNAseq_biodiversa/dataset/v1$ zless fastq.featurecounts.ribo.ex.count.gz | grep "^H_" | cut -f 7 | bawk 'BEGIN{n=0}{n=n+$1}END{print n}'
# 8608885
# (bit_rnaseq_2.8) egrassi@godot:/mnt/trcanmed/bioinfotree/prj/RNAseq_biodiversa/dataset/v1$ zless fastq.featurecounts.ribo.ex.count.gz | grep "^M_" | cut -f 7 | bawk 'BEGIN{n=0}{n=n+$1}END{print n}'
# 232333

# sample  tot     unmap   ribo    non_ribo_multi_map      non_ribo_uniq_map       Assigned        Unassigned_Ambiguity    Unassigned_NoFeatures
rule n_reads:
    input: 'merged_batch.tsv','merged_hs_mm.tsv.gz', [BASE_DIR+'/'+USABLE_READS[b] for b in list(USABLE_READS.keys())]
    output: matrix='reads_info.tsv.gz'
    run:
        import pandas as pd
        batch = pd.read_csv(input[0], sep="\t", header=None)
        batch.columns = ['id','batch']
        counts = pd.read_csv(input[1], sep="\t", index_col=0)
        # gather total mouse and human reads
        h_counts = counts.filter(regex="^H_", axis=0).sum(axis=0)
        m_counts = counts.filter(regex="^M_", axis=0).sum(axis=0)
        count_res = pd.DataFrame(data={'h_tot':h_counts, 'm_tot':m_counts, 'frac_h' : h_counts/(m_counts+h_counts)})
        usable_reads_res = pd.DataFrame()
        b = 1
        for ur in input[2:]:
            print(ur)
            usable_reads = pd.read_csv(ur, sep="\t", index_col=0, usecols=['sample', 'tot','Assigned'])
            usable_reads['frac_assigned'] = usable_reads['Assigned']/usable_reads['tot']
            wanted = batch[batch['batch'] == b].id
            wantedrows = usable_reads.index.intersection(wanted)
            usable_reads_res = usable_reads_res.append(usable_reads.loc[wantedrows])
            b += 1
        res = pd.concat([count_res, usable_reads_res], axis=1, sort=True)
        res.to_csv(output.matrix, sep="\t", compression='gzip', header=True)


rule suppl_qc:
    input: qc="reads_info.tsv.gz", meta="selected_metadata_annot_final"
    output: "rnaseq_samples.xlsx"
    shell: 
        """
            cut -f 1,2,3,4,5,6,7 {input.meta} > {output}.tmp
            zcat {input.qc} > {output}.tmp2
            tsv_to_xls -i {output}.tmp,{output}.tmp2 -s samples_annotations,reads_qc -o {output} 
            rm {output}.tmp*
        """

rule suppl_qc_noP:
    input: qc="reads_info.tsv.gz", meta="selected_metadata_annot_final"
    output: "rnaseq_samples_noP.xlsx"
    shell: 
        """
            cut -f 1,2,3,4,5,6,7 {input.meta} | grep -v PR > {output}.tmp
            zcat {input.qc} | grep -v PR > {output}.tmp2
            tsv_to_xls -i {output}.tmp,{output}.tmp2 -s samples_annotations,reads_qc -o {output} 
            rm {output}.tmp*
        """

# selected metadata filtered based on n reads with thr on frac_h and frac_assigned
rule remove_samples_readsQC:
    input: 'reads_info.tsv.gz'
    output: 'removed_samples_readsQC'
    params: thr_frac_h=THR_FRAC_H, thr_frac_assigned=THR_FRAC_ASSIGNED
    shell:
        """
            zcat {input} | sed 1d | bawk '$4 < {params.thr_frac_h} || $7 < {params.thr_frac_assigned} {{print $1}}' > {output}
        """
 
# TODO last filtering step on selected_metadata_annot_final_nolinfo_nooutlier


### Kat/Franziska phospo-P
rule KF_samples:
    input: meda="selected_metadata_annot_final_nolinfo_nooutlier", filter=PHO
    output: "phospho_72_lmx_july2020_metadata.tsv"
    shell:
        """
            cat {input.meda} |  bawk 'NR == 1' > {output}
            cat {input.meda} | filter_1col 1 <(cut -f2 {input.filter}) >> {output}

        """

### This one could filter samples too, but maybe we want to have the whole counts first?
rule genes:
    input: "gene_len", "merged_hs_mm.tsv.gz"
    output: "H_gene_len.tsv", "H_readcount.tsv.gz"
    run:
        import pandas as pd
        leng = pd.read_csv(input[0], sep="\t")
        counts = pd.read_csv(input[1], sep="\t", compression='gzip')
        leng = leng[~leng.Geneid.str.contains("^M_")]
        counts = counts[~counts.Geneid.str.contains("^M_")]
        leng.Geneid = leng.Geneid.str.slice(2)
        counts.Geneid = counts.Geneid.str.slice(2)
        leng.to_csv(output[0], sep='\t', header=True, index=False)	
        counts.to_csv(output[1], sep='\t', header=True, index=False, compression='gzip')	

rule filter_samp:
    input: "H_readcount.tsv.gz", "phospho_72_lmx_july2020_metadata.tsv"
    output: "phospho_72_lmx_july2020_readcount.tsv.gz"
    run:
        import pandas as pd
        df = pd.read_csv(input[0], sep="\t", compression='gzip', index_col=0)
        filt = pd.read_csv(input[1], sep="\t")
        # filt = filt.rename(columns={'sample_id_R':'Geneid'})
        # df = df.loc[:, ~df.columns.isin(filt.index)]
        df = df.loc[:, df.columns.isin(filt.sample_id_R)]
        # df = df[[~df.columns.isin(filt.index)]]
        df.to_csv(output[0], sep='\t', header=True, index=True, compression='gzip')

### add cetuxi in a meaningful way from pdxopedia, check vs old then remove old
rule cetuxi:
    input: meta="selected_metadata_annot_final_nolinfo_nooutlier_replisafe", ctx=CTX
    output: "selected_metadata_annot_final_nolinfo_nooutlier_ctx"
    shell:
        """
            head -n 1 {input.meta} | bawk '{{print $0,"model","ctx_3w"}}' > {output}
            sed 1d {input.meta} | bawk '{{print $0,substr($1,0,10)}}' | translate -a -v -e NA <(bawk '{{print $1"LMX",$2 }}' {input.ctx}) 10 >> {output}
        """
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/july2020_starOK$ grep LMX selected_metadata_annot_final_nolinfo_nooutlier_ctx |bawk '$8!= "NA" && $8*100 != $11'
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/july2020_starOK$ grep LMX selected_metadata_annot_final_nolinfo_nooutlier_ctx |bawk '$8!= "NA"' | wc -l
#462

rule metadata_filtered:
    input: "selected_metadata_annot_final_nolinfo_nooutlier_ctx"
    output: "filtered_metadata_ctx.tsv"
    shell:
        """
            cut -f 1,6,7,11 {input} > {output}
        """

rule basali:
    input: meda="filtered_metadata_ctx.tsv", right=RIGHT
    output: "filtered_metadata_ctx_basali_human-org-xeno"
    shell:
        """
            cat {input.meda} | bawk 'NR==1' > {output}
            cat {input.meda} | filter_1col 3 {input.right} >> {output}
        """

# mviviani@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/july2020_starOK$ cat filtered_metadata_ctx_basali_human-org-xeno | bawk '{print $3}' | sed 1d | sort | uniq
# LMH
# LMO_BASALE
# LMO_BASALE.1
# LMO_BASALE.2
# LMX_BASALE
# LMX_BASALE.1
# PRH
# PRX_BASALE
# PRX_BASALE.1
rule filtered_metadata_repli_safe:
    input: "selected_metadata_annot_final_nolinfo_nooutlier"
    output: "selected_metadata_annot_final_nolinfo_nooutlier_replisafe"
    shell:
        """
           perl -ane '$F[0] =~ s/\-/\./; print join("\\t", @F); print "\\n"' < {input} > {output} 
        """


# We get the tss info from FeatureCounts output ditching out genes mapping on different chrs/strands
rule tss:
    input: "merged_info.tsv"
    output: "tss.tsv"
    run:
        import pandas as pd
        import numpy as np
        data = pd.read_csv(input[0], sep="\t")
        chrs = data['Chr'].str.split(';').map(lambda x: x[0] if len(set(x)) == 1 else None)
        toremove = chrs.apply(lambda x: x is None) # 173
        toremove2 = np.full(len(data), False)
        tss = np.zeros(len(data)) 
        strand = np.full(len(data), "0") 
        for i in range(0, len(data)):
            strands = data['Strand'][i].split(';')
            if len(set(strands)) == 1:
                toremove2[i] = False
                if strands[0] == "+":
                    tss[i] = int(data['Start'][i].split(';')[0])
                    strand[i] = "+"
                elif strands[0] == "-":
                    tss[i] = int(data['End'][i].split(';')[-1])
                    strand[i] = "-"
                else:
                    raise Exception('Unknown strand ' +  strands + " in " + str(i))
            else:
                toremove2[i] = True
        # We filter out genes with different chrs or different strands, there are few of them (185)
        toremove_all = np.logical_or(pd.Series(toremove2), toremove)
        res = pd.DataFrame(data={'Geneid': data['Geneid'], 'chr': chrs, 'tss': tss, 'strand': strand})
        res = res.loc[np.logical_not(toremove_all)]
        res.to_csv(output[0], sep="\t", index=False)

# We get -up +down around TSS converting 1 based end included coords to bed (0 based end excluded)
rule around_tss:
    input: "tss.tsv"
    output: "{up}_{down}_tss.tsv"
    shell:
        """
            sed 1d {input} | sed 's/Hchr/chr/1' | bawk '$4=="+" {{print $2, $3 - {wildcards.up} - 1, $3 + {wildcards.down}, $1}} \\
                                                        $4=="-" {{print $2, $3 - {wildcards.down} - 1, $3 + {wildcards.up}, $1 }}' > {output}
        """