# Use snakemake metadata_forall  to check if you have basic metadata available for all the samples in the batches
# that you want to analyze.
include: "./conf.sk"

rule wide_to_long_metadata:
    input: OVERVIEW
    output: "metadata"
    params: tool=BIN_DIR+"/overview2long"
    shell:
        """
            {params.tool} {input} {output}
        """


rule has_metadata:
    input: "metadata"
    output: "has_metadata"
    shell:
        """
            cut -f 2 {input} | sort | uniq > {output}
        """

rule batches:
    input: BATCHES_FASTQS
    output: "fastq_batches"
    run:
        batch = 1
        with open(output[0], 'w') as out:
            for f in input:
                with open(f,'r') as inf:
                    for l in inf.readlines():
                        l = l.rstrip("\n")
                        l = l.replace('_R1.fastq.gz','')
                        out.write(l + "\t" + str(batch) + "\n")
                    batch += 1            

rule info_and_fastq:
    input: meta="metadata", fastq="fastq_batches"
    output: "complete_samples_batch"
    shell:
        """
            cat {input.fastq} | filter_1col 1 <(cut -f 2 {input.meta}) > {output}
        """

rule metadata_forall:
    input: "complete_samples_batch", "fastq_batches"
    shell:
        """
            n1=$(cat {input[0]} | wc -l);
            n2=$(cat {input[1]} | wc -l);
            if [ "$n1" -eq "$n2" ]; then
                echo "Good to go!"
            else
                echo "Go back to square one!"
            fi;
        """

# Note: for batch correction we want the smaller batch a genealogy appears (probably...for library prep batch effect honestly we will never be sure), 
# but for alignment we want the higher one
## Progressive enrichment of metadata
rule metadata_batch:
    input: batch="complete_samples_batch", metadata="metadata"
    output: "metadata_batch"
    shell:
        """
            translate -a -n {input.metadata} 1 < {input.batch} > {output}
        """

# TODO rule add depth? HOW? Ask Franceschino or Claudio for the list.

rule add_cetuxi:
    input: start="metadata_batch", add=DATA_DIR+"/cetuxi_w3.txt"
    output: "metadata_batch_w3vivocetuxi"
    shell:
        """
            bawk '{{print $0,substr($1,0,7)}}' {input.start} | translate -v -e NA {input.add} 4 > {output}
        """

rule add_irino:
    input: start="metadata_batch_w3vivocetuxi", add=DATA_DIR+"/irinotecan_w3.txt"
    output: "metadata_batch_w3vivocetuxi_w3vivoirino"
    shell:
        """
            bawk '{{print $0,substr($1,0,7)}}' {input.start} | translate -v -e NA <(sed 1d {input.add} | bawk '{{print $1,$2/100}}') 5 > {output}
        """

# rule add_sanger:
#     input: start="metadata_batch_w3vivocetuxi_w3vivoirino", add=DATA_DIR+"/sanger.txt"
#     shell:
#         """
        
#         """

# rule add_vwes:

# rule add mutations from dtb_mutations?

# Will then select all PDO, all PDX and all human samples from the whole list to align all together? Downloading from different places will
# be complicated tough...
rule all_n_samples_comparisons:
    input: expand("batch_{b}.txt", b=list(BATCHES_COUNTS.keys()))

#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ cat batch_?.txt
# TODO manual check on results
rule n_samples_comparisons:
    input: meta="metadata_batch", counts=lambda wildcards: BASE_DIR+"/"+BATCHES_COUNTS[wildcards.b]
    output: "batch_{b}.txt"
    shell:
        """
            bawk '$3=={wildcards.b}' {input.meta} | wc -l > {output}
            zcat {input.counts} | bawk 'NR==1' | cut -f 7-  | tr "\\t" "\\n" | wc -l >> {output}
        """

rule choose_batch: 
    input: meta="metadata_batch"
    output: chosen_batch="merged_batch.tsv"
    run:
        import pandas as pd 
        batch = pd.read_csv(input.meta, sep="\t", header=None) # ERROR ADD HEADER_NONE @!@!!!! TODO
        batch.columns = ['id','class','batch']
        chosen = batch.groupby(['id'])['batch'].max()
        chosen.to_csv(output.chosen_batch, sep="\t")


rule matrix_hs_mm: 
    input: "merged_batch.tsv", [BASE_DIR+"/"+BATCHES_COUNTS[b] for b in list(BATCHES_COUNTS.keys())]
    output: matrix="merged_hs_mm.tsv.gz", info="merged_info.tsv"
    run:
        import pandas as pd 
        batch = pd.read_csv(input[0], sep="\t", header=None)
        batch.columns = ['id','batch']
        res = None
        b = 1
        for count in input[1:]:
            print(count)
            count = pd.read_csv(count, sep="\t", index_col=0)
            wanted = batch[batch['batch'] == b].id
            if b == 1:
                info = count[['Chr','Start','End','Strand','Length']]
            wantedcolumns = count.columns.intersection(wanted)
            #wantedcolumns.to_frame().to_csv(str(b)+"meh.tsv")
            #count.columns.to_frame().to_csv(str(b)+"meh1.tsv")
            #wanted.to_frame().to_csv(str(b)+"meh2.tsv")
            if b != 1:
                 # suffixes=(False, False): raise an exception on overlapping columns
                res = count[wantedcolumns].merge(res, suffixes=(False, False), left_index=True, right_index=True)
            else:
                res = count[wantedcolumns]
            b += 1
        res.to_csv(output.matrix, sep="\t", compression='gzip', header=True)
        info.to_csv(output.info, sep="\t")


rule merged_human:
    input: "merged_hs_mm.tsv.gz"
    output: "merged_hs.tsv.gz"
    shell:
        """
            zcat {input} | bawk 'NR==1' > {output}.tmp
            zcat {input} | grep "^H_" >> {output}.tmp
            gzip -c {output}.tmp > {output}
            rm {output}.tmp
        """

#rule check: done!
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ zcat merged_hs_mm.tsv.gz  | wc -l
#109888
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ zcat merged_hs_mm.tsv.gz  | bawk '{print NF}' |head
#937
#egrassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ cut -f 1 metadata_batch_w3vivocetuxi_w3vivoirino  |sort| uniq | wc -l
#938
# FIXED (idiota):
#grassi@godot:/mnt/trcanmed/snaketree/prj/RNASeq_biod_metadata/dataset/april2020$ zcat merged_hs_mm.tsv.gz  | bawk '{print NF}' |head
#939

rule selected_metadata:
    input: meta="metadata_batch_w3vivocetuxi_w3vivoirino", sel="merged_batch.tsv"
    output: "selected_metadata"
    shell:
        """
            bawk '{{print $1,$3,$2,$4,$5}}' {input.meta} | filter_2col --both-orders 1 2 {input.sel} > {output}
        """

rule gene_len:
    input: "merged_info.tsv"
    output: "gene_len"
    shell:
        """
            cut -f1,6 {input} | sed 's/Length/length/1' > {output}
        """

### Json for Las repoindex ###
#biod id	ID Vale	id	LAS_Validation	clone_N	cloning_date	species
rule json_meta:
    input: metadata="metadata_batch"
    output: "{b}_meta_for_json.tsv"
    shell:
        """
            echo -e "ID\\tLAS_Validation\\tlabel" > {output}
            grep -v '\-' {input.metadata} | bawk '$3=="{wildcards.b}" {{print $1,"TRUE",$2}}' >> {output}
            grep '\-' {input.metadata} | bawk '$3=="{wildcards.b}" {{print $1,"FALSE",$2}}' >> {output}
        """

rule json:
    input: tsv="{b}_meta_for_json.tsv", head=JSON_HEAD+"_{b}.json"
    output: "batch{b}_metadata.json"
    params: tool=BIN_DIR+"/tsv_to_json_metadata"
    shell:
        """
           cp {input.head} {output}
           {params.tool} -i {input.tsv} -j {output}
        """

rule all_json:
    input: expand("batch{b}_metadata.json", b=range(1, len(BATCHES_FASTQS)+1))


    
